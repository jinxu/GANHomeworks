{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Попередня обробка тексту для RNN:\n",
    "  * Завантажте текстовий корпус та перетворіть його у послідовність символів або слів.\n",
    "  * Створіть словник для перетворення символів або слів у числові індекси.\n",
    "  * Розділіть текст на короткі фрагменти (наприклад, послідовності по 100 символів або слів), які будуть використовуватися як вхідні дані для RNN.\n",
    "\n",
    "2. Створення архітектури RNN:\n",
    "  * Визначте рекурентну нейронну мережу, використовуючи шари LSTM або GRU (в залежності від обраної бібліотеки).\n",
    "  * Додайте один або більше рекурентних шарів з вибраною кількістю нейронів у кожному шарі.\n",
    "  * Додайте повнозв’язний шар із softmax для прогнозування ймовірностей наступних символів або слів."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ідея в примінені RNN GRU для генерації новин з різних джерел. Я всяв с Kaggle датасет з новинами. Зробив модель для кожного джерела."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['BBC Russia', 'Meduza', 'Novaya Gazeta', 'RBK', 'RIA News', 'Shocked Ukraine', 'TASS', 'The New York Times', 'Ukraine Now', 'Ukraine24', 'Washington Post', 'УНИАН'])\n"
     ]
    }
   ],
   "source": [
    "# Завантажте текстовий корпус та перетворіть його у послідовність символів або слів.\n",
    "df = pd.read_csv('data/news.csv', index_col=0)\n",
    "df=df.dropna()\n",
    "grouped_texts = df.groupby('group_name')['text'].apply(list).to_dict()\n",
    "print(grouped_texts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if MPS is available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "\n",
    "# Create vocabulary\n",
    "def build_vocab(df, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for text in df:\n",
    "        counter.update(text.split())\n",
    "    vocab = {word: idx + 1 for idx, (word, count) in enumerate(counter.items()) if count >= min_freq}\n",
    "    vocab['<PAD>'] = 0\n",
    "    vocab['<UNK>'] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# Custom Dataset\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, vocab, seq_length=30):\n",
    "        self.vocab = vocab\n",
    "        self.seq_length = seq_length\n",
    "        self.word_to_idx = vocab\n",
    "        self.idx_to_word = {idx: word for word, idx in vocab.items()}\n",
    "        self.data = self.prepare_sequences(texts)\n",
    "        \n",
    "    def prepare_sequences(self, texts):        \n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            tokens = text.split()\n",
    "            idx_sequence = [self.word_to_idx.get(word, self.word_to_idx['<UNK>']) for word in tokens]\n",
    "            sequences.append(idx_sequence)\n",
    "        return sequences\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.data[idx]\n",
    "        if len(sequence) > self.seq_length:\n",
    "            start_idx = np.random.randint(0, len(sequence) - self.seq_length)\n",
    "            sequence = sequence[start_idx:start_idx + self.seq_length]\n",
    "        else:\n",
    "            sequence = sequence + [self.word_to_idx['<PAD>']] * (self.seq_length - len(sequence))\n",
    "        \n",
    "        x = torch.tensor(sequence[:-1])\n",
    "        y = torch.tensor(sequence[1:])\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Model (more memory efficient than LSTM)\n",
    "class TweetGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        gru_out, _ = self.gru(embed)\n",
    "        return self.fc(gru_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,dataloader,optimizer,criterion,vocab,num_epochs):\n",
    "    # Training loop    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x)\n",
    "            loss = criterion(output.reshape(-1, len(vocab)), batch_y.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "    return model\n",
    "\n",
    "    # Generate text\n",
    "def generate_tweet(model, vocab, seed_text=\"hello\", max_length=30):\n",
    "    model.eval()\n",
    "    word_to_idx = vocab\n",
    "    idx_to_word = {idx: word for word, idx in vocab.items()}\n",
    "    \n",
    "    words = seed_text.split()\n",
    "    current_ids = torch.tensor([word_to_idx.get(w, word_to_idx['<UNK>']) for w in words]).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(words)):\n",
    "            output = model(current_ids)\n",
    "            next_word_idx = torch.argmax(output[0, -1]).item()\n",
    "            words.append(idx_to_word[next_word_idx])\n",
    "            current_ids = torch.cat([current_ids, torch.tensor([[next_word_idx]]).to(device)], dim=1)\n",
    "            \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBC Russia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [18:31<00:00,  3.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300, Loss: 0.1788\n",
      "Украина будет создавать для России постоянно <UNK> и абсолютно неприемлемую <UNK> - сказал Путин в четверг в телеобращении в котором <UNK> объявил о начале военных действий против <UNK> <UNK> 2021\n",
      "Зеленский – <UNK> <UNK> с нашей <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Путин о начале \"специальной военной операции\" в Донбассе. Главное <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Ukraine24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [16:56<00:00,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300, Loss: 0.1327\n",
      "Украина <UNK> <UNK> <UNK> <UNK> <UNK> - <UNK> в <UNK> <UNK> <UNK> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Зеленский созвонился с <UNK> Украину в <UNK> но и в <UNK> <UNK> <UNK> страну сделали <UNK> <UNK> <UNK> с <UNK> <UNK> <UNK> <UNK> цвета <UNK> <UNK> <UNK> в этом <UNK>\n",
      "Путин заявил, что начинает военную операцию в Украине. Он <UNK> что в его <UNK> нет оккупации - а только <UNK> и <UNK> . Около 5 утра в районе аэропорта <UNK>\n",
      "УНИАН\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [17:32<00:00,  3.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300, Loss: 0.2095\n",
      "Украина была и остается <UNK> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Зеленский сообщает, что провел переговоры с <UNK> <UNK> и <UNK> <UNK> <UNK> что <UNK> <UNK> <UNK> <UNK> санкции и <UNK> и военная поддержка <UNK> должен принудить РФ к миру\" <PAD>\n",
      "Путин готов <UNK> в <UNK> с <UNK> вы <UNK> на <UNK> <UNK> й <UNK> <UNK> <UNK> с <UNK> що <UNK> <UNK> в <UNK> області у місті <UNK> ВЧ <UNK> <UNK>\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "vocabs = {}\n",
    "num_epochs=300\n",
    "n_tweets = 1000\n",
    "\n",
    "selected_sources = ['BBC Russia','УНИАН','Ukraine24']\n",
    "for source,text in grouped_texts.items():\n",
    "    if source not in selected_sources:\n",
    "        continue\n",
    "    print(source)\n",
    "    # print(f'Size of train data: {len(text)}')\n",
    "    # text = random.sample(text, k=n_tweets) \n",
    "    text = text[:n_tweets]\n",
    "\n",
    "    vocab = build_vocab(text)\n",
    "    vocabs[source]=vocab #if do one vocab to all model, then i got memory error\n",
    "    dataset = TweetDataset(text, vocab)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    model = TweetGenerator(len(vocab)).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    models[source]=train(model=model, \n",
    "                        dataloader=dataloader, \n",
    "                        optimizer=optimizer,\n",
    "                        criterion=criterion, \n",
    "                        num_epochs=num_epochs,\n",
    "                        vocab=vocab)\n",
    "    \n",
    "    print(generate_tweet(models[source], vocabs[source], seed_text=\"Украина\"))\n",
    "    print(generate_tweet(models[source], vocabs[source], seed_text=\"Зеленский\"))\n",
    "    print(generate_tweet(models[source], vocabs[source], seed_text=\"Путин\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
